{
  
    
        "post0": {
            "title": "Part 2: GeoFLOW - Automate GeoSpatial workflow using Notebook & Papermill",
            "content": ". Problems with Notebook . In part 1 of the GeoFLOW series, we created a GeoSpatial workflow using Python &amp; Jupyter Notebook. Looking at the intermediary visuals is certainly useful but there are a few drawbacks in creating such workflows: . Version Controlling Notebooks are difficult to version control, we can eliminate a part of the pain using nbdime but still, you will add up to your commits every time you open a notebook or click on a cell. | Scalability For every city you wish to repeat the workflow, you have to manually run the cells one at a time. The next alternative is to move everything to a python script &amp; save the images to inspect later. (I don’t like either) | Can we do any better? . Papermill gives us the superpowers to parameterize &amp; execute the notebooks. . How does this solve our problems: . Version Controlling We always clean the outputs of the parameterized notebook before committing the code. This keeps our workflow &amp; commit messages sane. | Scalability Now we can execute notebooks by running a python scripts, it will have our notebooks ready &amp; baked for further visual analysis. | This workflow also makes it easier for us to catch bugs &amp; fix them in place. (Try them out yourself, don’t just take my word for it) . How to use this technique? . We will repeat the same process that we had defined in Part 1 of this series but now we will repeat it for multiple cities, i.e Fortportal &amp; Entebbe. . Steps . Parameterize the required cell in the notebook . . | Define a configuration file config.json that would replace the parameters cell (i.e the cell we tagged in step 1) with the values with which we want to execute the notebook. . [ { &quot;REGION&quot;: &quot;fortportal&quot;, &quot;UTM&quot;: 32636, &quot;PIPELINE&quot;: &quot;gridded-population&quot; }, { &quot;REGION&quot;: &quot;entebbe&quot;, &quot;UTM&quot;: 32636, &quot;PIPELINE&quot;: &quot;gridded-population&quot; } ] . | Add an orchestrator.py to orchestrate the workflow . def process(region, pipeline): &#39;&#39;&#39;Processes the pipeline for a given region &amp; stores the executed notebook inside nb-output/ folder. Input - region: Name of the region - pipeline: Pipeline to run (gridded-population / cluster / query-engine) Return None &#39;&#39;&#39; def execute(nb, pipeline, region): &#39;&#39;&#39;Execute a notebook using papermill.&#39;&#39;&#39; pm.execute_notebook( input_path=nb, output_path=f&#39;nb-output/{pipeline}-executed-{region[&quot;REGION&quot;]}_@_{dt.now().strftime(&quot;%Y-%m-%d_%I-%M-%S_%p&quot;)}.ipynb&#39;, parameters=region ) if pipeline == &#39;query&#39; : execute(&#39;2020-11-06-query-engine.ipynb&#39; , pipeline, region) elif pipeline == &#39;cluster&#39; : execute(&#39;2020-11-06-cluster.ipynb&#39; , pipeline, region) elif pipeline == &#39;gridded-population&#39;: execute(&#39;2020-11-06-gridded-population.ipynb&#39;, pipeline, region) else: print(&#39;Not a valid pipeline&#39;) . | Run the orchestrator.py file . python orchestrator.py . | . This will execute the parameterized notebooks &amp; store the executed notebooks inside nb-output/ folder. . Fortportal . . Entebbe . . Try this workflow &amp; suggest any changes to improve it further. Happy Coding! .",
            "url": "https://srmsoumya.github.io/blog/gis/2020/11/07/automate-geoflow.html",
            "relUrl": "/gis/2020/11/07/automate-geoflow.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 1: GeoFLOW - Geospatial workflow using Python & Jupyter Notebook",
            "content": ". Where do people live? . Population data is available at a low spatial &amp; temporal resolution. For most of the countries around the world, it is done once every 10 years &amp; available at a ward level at best. . Government, NGOs &amp; other public bodies design a lot of policies &amp; measures based on this outdated dataset. In the event of an endemic or natural disaster, it would help a lot if you know exactly where people live &amp; track their movements. . To address this issue, we will try to estimate the human population at 100m x 100m GRID level. Using Building footprints as a proxy for human settlement, we will see how to distribute a global data source like Gridded Population of the World (GPW) can be used to estimate the human population at such granular resolution. . Details of the city under consideration . We are looking at the city of Fortportal, which is regarded as the tourism city in Uganda . REGION = &#39;fortportal&#39; UTM = 32636 PIPELINE = &#39;output&#39; . Load the city boundary GeoJSON file . region = gpd.read_file(INPUT/f&#39;{REGION}.geojson&#39;).to_crs(epsg=WGS84) box = region.total_bounds show(region) . Load Population data . Read the Gridded Population of the World (GPW) dataset, which is available as a Raster TIFF file | Clip it to the city boundary using the GeoJSON file | Save the clipped raster | . gpw = rio.open(INPUT/&#39;gpw_world.tif&#39;) gpw_region = gpw.read(1, window=gpw.window(*box)) # Clip to the region boundary region_mask, region_mask_tfm = mask(dataset=gpw, shapes=region.geometry, all_touched=True, crop=True, filled=True) region_mask = np.where(region_mask &lt; 0, 0, region_mask).squeeze() # Save the clipped raster region_meta = gpw.meta region_meta.update(dict( driver=&#39;GTiff&#39;, height=region_mask.shape[0], width=region_mask.shape[1], transform=region_mask_tfm )) with rio.open(INTER/f&#39;{REGION}_gpw_output.tif&#39;, &#39;w&#39;, **region_meta) as f: f.write(region_mask, indexes=1) show(region, region_mask) . Polygonize the clipped raster . Population data for the city is now available as a Raster TIFF file. To read it as a GeoDataFrame &amp; perform spatial operations, we would need to polygonize the raster into a vector format. . We can do this using the rasterio library in python. . . Tip: Use rasterio CLI inside jupyter notebooks, save a few lines of pythonic code . !rio shapes {INTER/REGION}_gpw_output.tif --bidx 1 --precision 6 &gt; {INTER/REGION}_gpw_output.geojson . tile_idx tile_population geometry tile_width tile_height . 0 0 | 288.332520 | POLYGON ((3366486.971 77925.582, 3366486.971 7... | 1 | 1 | . 1 1 | 521.265198 | POLYGON ((3367414.596 77925.582, 3367414.596 7... | 1 | 1 | . 2 2 | 780.061462 | POLYGON ((3368342.222 77925.582, 3368342.222 7... | 1 | 1 | . 3 3 | 1066.110474 | POLYGON ((3369269.958 77925.582, 3369269.958 7... | 1 | 1 | . 4 4 | 253.477982 | POLYGON ((3370197.584 77925.582, 3370197.584 7... | 1 | 1 | . # of tiles: 68, Area covered: 61.96 km.sq Population statistics: mean: 1068.56, median: 1025.65, std: 547.29 . Gridify . The vectorized GPW GeoJSON file gives us the population for the city at 1km x 1km spatial resolution. . The intent is to estimate the population at 100m x 100m spatial resolution. To achieve this, we have to: . Gridify: Split the 1km x 1km TILE into smaller grids of size 100m x 100m | Each 1km x 1km TILE will give us 100 GRIDs of size 100m x 100m | . . Tip: Gridification works well only for the MERCATOR projection, make sure your GeoDataFrames have proper CRS before performing this operation. . def gridify(tile): polygons = [] xmin,ymin,xmax,ymax = tile.geometry.bounds width = tile.tile_width height = tile.tile_height stepx = +(xmax - xmin)/(10 * width ) stepy = -(ymax - ymin)/(10 * height) for x in np.arange(xmin, xmax, stepx): for y in np.arange(ymax, ymin, stepy): poly = [ (x , y ), (x + stepx, y ), (x + stepx, y + stepy), (x , y + stepy) ] polygons.append(Polygon(poly)) d = { &#39;geometry&#39;: polygons, &#39;tile_idx&#39;: tile.tile_idx, &#39;tile_population&#39;: tile.tile_population, &#39;tile_width&#39;: tile.tile_width, &#39;tile_height&#39;: tile.tile_height } grids_gdf = gpd.GeoDataFrame(d, crs=f&#39;EPSG:{MERCATOR}&#39;) tile_gdf = gpd.GeoDataFrame(tile.to_frame().T, crs=f&#39;EPSG:{MERCATOR}&#39;) grids_gdf = gpd.clip(grids_gdf, tile_gdf) return grids_gdf # For each TILE create the GRIDs grids_gdf = tiles_gdf.apply(gridify, axis=1) grids_gdf = pd.concat(grids_gdf.to_list()) grids_gdf = grids_gdf.reset_index(drop=True).reset_index().rename(columns={&#39;index&#39;: &#39;idx&#39;}).to_crs(epsg=UTM) # Change the CRS of TILEs &amp; GRIDs back to their region respective UTM coordinates tiles_gdf = tiles_gdf.to_crs(epsg=UTM) grids_gdf = grids_gdf.to_crs(epsg=UTM) region = region.to_crs(epsg=UTM) grids_gdf.head() . idx geometry tile_idx tile_population tile_width tile_height . 0 0 | POLYGON ((192969.965 77461.414, 193062.792 774... | 0 | 288.33252 | 1 | 1 | . 1 1 | POLYGON ((192969.911 77369.202, 193062.738 773... | 0 | 288.33252 | 1 | 1 | . 2 2 | POLYGON ((192969.857 77276.990, 193062.684 772... | 0 | 288.33252 | 1 | 1 | . 3 3 | POLYGON ((192969.803 77184.777, 193062.630 771... | 0 | 288.33252 | 1 | 1 | . 4 4 | POLYGON ((192969.749 77092.565, 193062.576 770... | 0 | 288.33252 | 1 | 1 | . Load building footprints dataset . We are using buildings as a proxy for estimating the human population. . Microsoft has been kind of to release building footprints for multiple regions around the globe, please check them out HERE. Fortunately, the city we are considering here (Fortportal in Uganda) is also made available by them. . If you are considering a region for which there are no readily available building footprints, here are a few alternatives: . Check Open Street Map (OSM), they have good coverage of a lot of places around the globe. Also, consider contributing to that as well | Put your deep learning skills in practice, try getting hands on some satellite imagery of the region &amp; generate building footprints using Segmentation models. Dave luo has a nice blog post about the same LINK. | . footprints_gdf = (gpd.read_file(f&#39;{INPUT/REGION}_footprints.geojson&#39;) .to_crs(epsg=UTM) .assign(centroid=lambda x: x.centroid, building_area=lambda x:x.geometry.area) .rename(columns={&#39;geometry&#39;: &#39;building_count&#39;}) .set_geometry(&#39;centroid&#39;)) footprints_gdf.head() . building_count centroid building_area . 0 MULTIPOLYGON (((194684.966 75640.197, 194680.2... | POINT (194676.911 75641.532) | 128.630075 | . 1 MULTIPOLYGON (((195342.053 76164.558, 195341.3... | POINT (195339.047 76167.382) | 33.503853 | . 2 MULTIPOLYGON (((196345.656 75470.388, 196343.7... | POINT (196347.957 75471.430) | 112.972240 | . 3 MULTIPOLYGON (((196429.562 75712.346, 196424.1... | POINT (196424.420 75711.871) | 52.979150 | . 4 MULTIPOLYGON (((196340.025 76339.930, 196346.4... | POINT (196341.162 76345.245) | 59.647773 | . Compute building statistics (# of buildings, area occupied by buildings) for each GRID . Now, we have: . GRIDs at 100m x 100m resolution | Building footprints for the city | . We would like to find the buildings falling under each GRID &amp; then compute their count &amp; area occupancy. We can do a spatial join between the two DataFrames, which is much faster thanks to geopandas &gt; 0.8. We take the centroid of each building footprints &amp; do a spatial join with the GRID boundaries. . def sjoin_polygon_footprints(poly_gdf, footprints_gdf, idx, name, agg): poly_gdf = gpd.sjoin(poly_gdf, footprints_gdf, how=&#39;left&#39;, op=&#39;intersects&#39;) poly_gdf = (poly_gdf.groupby(idx) .agg(agg) .reset_index()) poly_gdf = (gpd.GeoDataFrame(poly_gdf, crs=f&#39;EPSG:{UTM}&#39;) .rename(columns={&#39;building_area&#39; : f&#39;{name}_building_area&#39;, &#39;building_count&#39;: f&#39;{name}_building_count&#39;})) return poly_gdf agg = { &#39;geometry&#39; : &#39;first&#39;, &#39;building_area&#39; : &#39;sum&#39;, &#39;building_count&#39; : &#39;count&#39;, &#39;tile_idx&#39; : &#39;first&#39;, &#39;tile_population&#39; : &#39;first&#39;, &#39;tile_width&#39; : &#39;first&#39;, &#39;tile_height&#39; : &#39;first&#39; } grids_gdf = sjoin_polygon_footprints(grids_gdf, footprints_gdf, &#39;idx&#39;, &#39;grid&#39;, agg=agg) grids_gdf.head() . idx geometry grid_building_area grid_building_count tile_idx tile_population tile_width tile_height . 0 0 | POLYGON ((192969.965 77461.414, 193062.792 774... | 0.0 | 0 | 0 | 288.33252 | 1 | 1 | . 1 1 | POLYGON ((192969.911 77369.202, 193062.738 773... | 0.0 | 0 | 0 | 288.33252 | 1 | 1 | . 2 2 | POLYGON ((192969.857 77276.990, 193062.684 772... | 0.0 | 0 | 0 | 288.33252 | 1 | 1 | . 3 3 | POLYGON ((192969.803 77184.777, 193062.630 771... | 0.0 | 0 | 0 | 288.33252 | 1 | 1 | . 4 4 | POLYGON ((192969.749 77092.565, 193062.576 770... | 0.0 | 0 | 0 | 288.33252 | 1 | 1 | . Remove extra GRIDs that fall beyond the region boundary &amp; adjust the population accordingly . The GPW population data is available to us at 1km x 1km resolution, we are clipping that to the region boundary. When we split the TILE into GRIDs, along the edges there will be GRIDs that fall beyond the region boundary. However, we will not have building footprints available for areas outside our region boundary, so we remove the extra grids &amp; adjust the population count by taking a ratio of the # of grids that fall within the region boundary. . grids_gdf = gpd.sjoin(grids_gdf, region[[&#39;geometry&#39;]], how=&#39;inner&#39;, op=&#39;intersects&#39;).drop(labels=&#39;index_right&#39;, axis=1) # Fix the index of the grids grids_gdf = (grids_gdf.drop(labels=&#39;idx&#39;, axis=1) .reset_index(drop=True).reset_index() .rename(columns={&#39;index&#39;: &#39;idx&#39;})) # Adjust the population accordingly def recompute_population_by_grids(gp): if gp.grid_building_count.sum() &lt;= 1: gp.tile_population = 0 return gp.tile_population * (gp.shape[0] / (gp.tile_width * gp.tile_height * 100)) grids_gdf[&#39;tile_population&#39;] = (grids_gdf.groupby(&#39;tile_idx&#39;) .apply(recompute_population_by_grids) .values) show(region, grids_gdf) . . idx geometry grid_building_area grid_building_count tile_idx tile_population tile_width tile_height . 0 0 | POLYGON ((193062.306 76631.449, 193155.133 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | . 1 1 | POLYGON ((193155.133 76631.395, 193247.960 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | . 2 2 | POLYGON ((193247.960 76631.341, 193340.787 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | . 3 3 | POLYGON ((193340.787 76631.287, 193433.613 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | . 4 4 | POLYGON ((193433.613 76631.233, 193526.440 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | . Compute the Population at 100m x 100m GRID level . For each grid, we have the TILE population, # of buildings inside its boundary &amp; the area of building inside its boundary. . We can statistically distribute the TILE population to the GRIDs by considering the building properties inside them. Like we discussed, buildings are a good proxy for the human population. . # Tile population distributed by the ratio of # of buildings for each grid grid_population_count = tile_population * (grid_building_count / tile_building_count) # Tile population distributed by the ratio of area of buildings for each grid grid_population_area = tile_population * (grid_building_area / tile_building_area ) grid_population = Weighted average of the above 2 metrics . . Tip: If you would like to improve over this model, conside using DEM &amp; DSM dataset, to find building heights &amp; use it as a factor to distribute the population . grids_gdf[[&#39;tile_building_count&#39;, &#39;tile_building_area&#39;]] = grids_gdf.groupby(&#39;tile_idx&#39;)[[&#39;grid_building_count&#39;, &#39;grid_building_area&#39;]].transform(&#39;sum&#39;) # Statistically distribute the TILE population to GRIDs grids_gdf[&#39;grid_population&#39;] = (0.5 * grids_gdf[&#39;tile_population&#39;] * (grids_gdf[&#39;grid_building_count&#39;] / grids_gdf[&#39;tile_building_count&#39;]) + 0.5 * grids_gdf[&#39;tile_population&#39;] * (grids_gdf[&#39;grid_building_area&#39; ] / grids_gdf[&#39;tile_building_area&#39; ])) grids_gdf.loc[:, &#39;grid_population&#39;] = grids_gdf[&#39;grid_population&#39;].fillna(0) grids_gdf.head() . idx geometry grid_building_area grid_building_count tile_idx tile_population tile_width tile_height tile_building_count tile_building_area grid_population . 0 0 | POLYGON ((193062.306 76631.449, 193155.133 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | 2 | 346.301592 | 0.0 | . 1 1 | POLYGON ((193155.133 76631.395, 193247.960 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | 2 | 346.301592 | 0.0 | . 2 2 | POLYGON ((193247.960 76631.341, 193340.787 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | 2 | 346.301592 | 0.0 | . 3 3 | POLYGON ((193340.787 76631.287, 193433.613 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | 2 | 346.301592 | 0.0 | . 4 4 | POLYGON ((193433.613 76631.233, 193526.440 766... | 0.0 | 0 | 0 | 25.949927 | 1 | 1 | 2 | 346.301592 | 0.0 | . Let us look at how the population distribution varies at 1km &amp; 100m resolution . Save the 100m x 100m GRID DataFrame . . Tip: If you need the data for further processing, consider using the feather format available in geopandas, it speeds up the reading &amp; writing of GeoJSON files. . grids_gdf.to_crs(WGS84).to_file(f&#39;{OUTPUT/REGION}_grids_output_{WGS84}.geojson&#39;, driver=&#39;GeoJSON&#39;) . Hopefully, this gives you a fair idea of why &amp; how to use Notebooks to create GeoSpatial workflows. . In the next post, we will see how to automate notebook execution using papermill. .",
            "url": "https://srmsoumya.github.io/blog/gis/2020/11/06/gridded-population.html",
            "relUrl": "/gis/2020/11/06/gridded-population.html",
            "date": " • Nov 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Namaste! I am Soumya Ranjan Mohanty aka SRM. . I work as a Full Stack Data Scientist at Gramener, India. In my day job, I work at the intersection of AI, GIS &amp; Environment Conservation. . I have a passion for teaching &amp; am a huge believer in free &amp; quality education for all. On weekends, I spend my time mentoring &amp; teaching students about Data Science at Springboard. . I love eating pizza &amp; playing football! .",
          "url": "https://srmsoumya.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://srmsoumya.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}